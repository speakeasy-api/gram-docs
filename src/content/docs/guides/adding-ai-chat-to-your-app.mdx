---
title: "Add AI Chat to Your CRUD App"
description: "Learn how to add a chat interface to your existing CRUD application in four practical steps while keeping your current backend and business logic completely unchanged."
sidebar:
  order: 1
---

You built a CRUD app. Your users create tasks, update records, and filter through data. But they spend too much time navigating your UI for simple requests. What if instead of clicking through filters and forms, they could just ask for what they need?

"Show me all tasks assigned to Sarah that are due this week." "Move the client presentation task to done." "Create a task for the database migration due Friday."

This guide shows you how to add a chat interface to your existing CRUD application in four practical steps. Your users get natural language interaction, while you keep your current backend, security model, and business logic completely unchanged.

<video width="600" controls style="display: block; margin: 0 auto;">
  <source src="/videos/taskboard-show-tasks.mp4" type="video/mp4" />
    Your browser does not support the video tag.
</video>

We'll use the [TaskBoard example app](https://github.com/ritza-co/TaskBoard) as a demo. The main branch shows our starting point (CRUD app without chat), and the [completed-ai-chat branch](https://github.com/ritza-co/TaskBoard/tree/completed-ai-chat) shows the final result.

![Github branches](/img/blog/adding-ai-chat-to-your-app-guide/github-branches.png)

:::note[Live Demo]
Try a [live demo of the TaskBoard app with a chat assistant here](https://taskboard.abduldavids.co.za).
:::

### Adding an AI assistant to a task management app

Here's how we'll transform a standard CRUD application into one with natural language capabilities.

We have a working task management app with a standard structure:

```
TaskBoard/
├── taskboard/                 # Your existing Next.js app
│   ├── src/
│   │   ├── app/api/           # Your CRUD API routes
│   │   │   ├── items/         # Task CRUD operations
│   │   │   ├── login/         # User authentication
│   │   │   └── register/      # User registration
│   │   ├── components/        # React components
│   │   └── lib/               # Database & auth helpers
│   └── prisma/                # SQLite database
└── docker-compose.yml         # Container setup
```

Here's what the app looks like currently:

![The TaskBoard user interface](/img/blog/adding-ai-chat-to-your-app-guide/taskboard-interface.png)

Users interact with your app manually: clicking, typing and moving tasks. It works, but we want users to "feel the AGI" when interacting with their tasks.

### What we'll add

We'll add a small chat microservice and popup chat component to your existing app:

```
TaskBoard/
├── taskboard/                 # Your existing Next.js app (unchanged)
│   ├── src/
│   │   ├── app/api/
│   │   │   ├── items/
│   │   │   ├── chat/          # New: proxy to chat service
│   │   │   ├── login/
│   │   │   └── register/
│   │   ├── components/
│   │   │   ├── KanbanBoard/
│   │   │   └── ChatComponent/ # New: floating chat popup
│   │   └── lib/
├── mcp-agent-service/         # New: small chat microservice
│   ├── main.py                # FastAPI + OpenAI Agents SDK
│   ├── requirements.txt       # 6 dependencies
│   └── Dockerfile             # Python container
└── docker-compose.yml         # Updated: runs both services
```

Here's what the app looks like with the chat component added:

![TaskBoard with chat functionality](/img/blog/adding-ai-chat-to-your-app-guide/taskboard-with-chat.png)

### How it works

For the chat service, we'll use [OpenAI's Agents SDK](https://openai.github.io/openai-agents-python/) to connect to your existing API. Here's the complete flow:

1. User types: "Show me overdue tasks"
2. Chat popup → Your Next.js API → Chat Service
3. OpenAI Agents SDK calls your existing `/api/items` endpoint
4. Response flows back with results formatted as natural language
5. TaskBoard UI updates to show the filtered results

![Using the TaskBoard chat to ask the agent to delete a task](/img/blog/adding-ai-chat-to-your-app-guide/taskboard-delete-task.png)

Your existing app stays exactly the same. Same database, same authentication, same business logic. You're just adding a new component to what you already built.

### Architecture details

Here's the complete architecture we're building:

![Diagram of the MCP-powered chat integration architecture](/img/blog/adding-ai-chat-to-your-app-guide/architecture-diagram.png)

**Before (manual interaction only)**:
- User clicks "Add Task" button → TaskBoard UI → POST /api/items → Database
- User drags task to "Done" → TaskBoard UI → PATCH /api/items → Database

**After (+ natural language chat)**:

*Option 1: Traditional UI (unchanged)*
- User clicks "Add Task" → TaskBoard UI → POST /api/items → Database → UI updates

*Option 2: Using the chat popup*
1. User types "Create a task for the client meeting tomorrow"
2. Chat UI → /api/chat → FastAPI Chat Service → mcp-agent
3. Gram MCP Server → POST /api/items → Database
4. Success response → Chat Service → Chat UI shows "✅ Task created"
5. TaskBoard UI automatically refreshes and shows the new task

Most importantly, your existing `/api/items` endpoints don't change at all. We're just adding a way to call the same APIs your UI already uses, but using an LLM to interpret the user's intent.

We use [MCP (Model Context Protocol)](https://modelcontextprotocol.io/docs/getting-started/intro) to provide the OpenAI Agent with specific tools that can call your TaskBoard API. Gram converts your API endpoints into MCP tools that the agent can use.

## Prerequisites and setup

To follow this guide, you'll need:

- A [Gram account](https://getgram.ai) (free)
- An OpenAI API key from the [OpenAI dashboard](https://platform.openai.com/account/api-keys)
- Node.js and Docker for local development
- The [TaskBoard repository](https://github.com/ritza-co/TaskBoard) cloned locally

### Clone and run the starter app

First, we'll get the basic CRUD app running without any chat functionality. The TaskBoard app is already dockerized for easy setup:

```bash
git clone https://github.com/ritza-co/TaskBoard.git
cd TaskBoard

# Make sure you're on the main branch (CRUD only, no chat)
git checkout main

# Create environment file
cp .env.example .env

# Start with Docker (recommended for testing)
docker-compose up --build
```

Visit `http://localhost:3000`, register an account, and create a few tasks. Click the arrows to move them between columns. This is our starting point, a working task management app that users interact with manually.

## Step 1: Creating an MCP server

First we'll give the OpenAI Agent tools that can call your API endpoints. MCP (Model Context Protocol) provides a standard way to expose API operations as tools that AI agents can use.

We'll upload your API documentation to Gram, which converts your API endpoints into MCP tools and hosts them as an MCP server. The OpenAI Agents SDK can then connect to this server and use these tools to interact with your TaskBoard API.

### Generate your API documentation

Your TaskBoard app already includes OpenAPI generation. Let's create the documentation file:

```bash
# In the taskboard directory
npm install
npm run generate-docs
```

This creates `/public/swagger.json` with your complete API specification, including authentication requirements and data schemas. The existing JSDoc comments in your API routes (like `/api/items/route.ts`) provide the structure Gram needs to understand your endpoints.

:::tip
If you have your own CRUD app, you likely already have an OpenAPI spec. If not, tools like [next-swagger-doc](https://www.npmjs.com/package/next-swagger-doc) or [swagger-jsdoc](https://www.npmjs.com/package/swagger-jsdoc) can generate one from your existing code comments. To learn more about how to generate an OpenAPI spec in other frameworks, check out our [OpenAPI hub](https://www.speakeasy.com/openapi) over on the Speakeasy blog.
:::

### Upload to Gram and create your MCP server

Now we'll transform your API documentation into a hosted MCP server:

**For first-time Gram users:**
1. In the Gram dashboard, click **Toolsets** in the sidebar (under **CREATE**).
2. Click **Get Started**.
3. Upload your OpenAPI document (`public/swagger.json`).
4. Name your API (e.g., "TaskBoard"), toolset, and server slug (e.g., "taskboard-demo").

<video width="600" controls style="display: block; margin: 0 auto;">
  <source src="/videos/taskboard-mcp-create.mp4" type="video/mp4" />
    Your browser does not support the video tag.
</video>

**For existing Gram users:**
1. Go to **Toolsets** in the sidebar.
2. In the **API Sources** section, click **+ ADD API**.
3. Upload your OpenAPI document (`public/swagger.json`).
4. Name the API (e.g., "TaskBoard") - tools are generated automatically.
5. Click **Continue**.
6. In the **Toolsets** section of the Toolsets tab, click **+ ADD TOOLSET**.
7. In the "Create a toolset" modal, give the toolset a name (e.g., "TaskBoard").
8. Click **Enable All** on the TaskBoard toolset page.

Gram parses your OpenAPI document and converts each endpoint into MCP tools. Find the MCP server in the **MCP** tab and set it to **Public** under the Visibility section.

![Gram MCP server](/img/blog/adding-ai-chat-to-your-app-guide/gram-public-mcp-server.png)

Copy the MCP server URL from the **MCP Installation** section shown in the screenshot above. You'll need the URL from the "args" array (like `https://app.getgram.ai/mcp/your-server-id`) for the chat service in Step 3.

### Configure public access for development

For local development, expose your TaskBoard API so Gram can access it:

```bash
# Install ngrok if you haven't already
brew install ngrok  # or download from ngrok.com

# In a new terminal, expose your local API
ngrok http 3000
```

Copy the public URL (like `https://abc123.ngrok.io`) and go to the **Environments** tab in the Gram sidebar. Add this URL to your **Default** environment as `TASKBOARD_SERVER_URL`.

![TaskBoard environment variables dialog](/img/blog/adding-ai-chat-to-your-app-guide/taskboard-env-var.png)

:::note[Production deployment]
We're using ngrok for local development testing. In production, you'd replace `TASKBOARD_SERVER_URL` with your actual API URL (like `https://api.taskboard.com`).
:::

### Test your MCP server

Verify everything works in the Gram playground:

1. Go to **Playground** in the sidebar (under **CONSUME**) → Select your TaskBoard toolset
2. Try queries like "Show me all my tasks" or "Create a task called 'Review quarterly reports'"

![Testing the TaskBoard MCP tools in the Gram Playground](/img/blog/adding-ai-chat-to-your-app-guide/testing-mcp-taskboard.png)

:::note
You'll need to provide a `userId` in the Playground, or ask the agent to use login/register tools to create a user or login to an existing account first. We'll automate this in the chat integration.
:::

Your API endpoints are now available as MCP tools!

## Step 2: Adding a chat popup to your frontend

Next we'll add a floating chat interface to your existing TaskBoard app. The chat window will appear as an overlay without interfering with your current functionality.

We're going to build a floating chat popup that looks like this:

![TaskBoard with chat functionality](/img/blog/adding-ai-chat-to-your-app-guide/taskboard-with-chat.png)

The chat component will be a floating button in the bottom-right corner that expands into a chat window when clicked. Let's build it step by step.

Create the chat component in your frontend:

```typescript
// taskboard/src/components/ChatComponent.tsx
import React, { useState, useRef, useEffect } from 'react';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  timestamp: string;
  toolUsage?: any;
  isStreaming?: boolean;
}

interface ChatComponentProps {
  userId: string;
  onChatClose?: () => void;
}

const ChatComponent: React.FC<ChatComponentProps> = ({ userId, onChatClose }) => {
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [sessionId, setSessionId] = useState<string | null>(null);
  const [isOpen, setIsOpen] = useState(false);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const textareaRef = useRef<HTMLTextAreaElement>(null);

  const MAX_MESSAGES = 5;
  const userMessageCount = messages.filter(msg => msg.role === 'user').length;

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  const adjustTextareaHeight = () => {
    const textarea = textareaRef.current;
    if (textarea) {
      textarea.style.height = 'auto';
      textarea.style.height = Math.min(textarea.scrollHeight, 100) + 'px';
    }
  };

  useEffect(() => {
    adjustTextareaHeight();
  }, [input]);

  const sendMessage = async () => {
    if (!input.trim() || isLoading) return;
    
    // Check message limit and show system message if reached
    if (userMessageCount >= MAX_MESSAGES) {
      const limitMessage: Message = {
        role: 'assistant',
        content: 'Message limit reached. Clear the conversation to continue.',
        timestamp: new Date().toISOString(),
      };
      setMessages(prev => [...prev, limitMessage]);
      return;
    }

    const userMessage: Message = {
      role: 'user',
      content: input.trim(),
      timestamp: new Date().toISOString(),
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);

    // Add streaming message placeholder
    const streamingMessage: Message = {
      role: 'assistant',
      content: '',
      timestamp: new Date().toISOString(),
      isStreaming: true,
    };
    setMessages(prev => [...prev, streamingMessage]);

    try {
      const response = await fetch(`/api/chat?userId=${encodeURIComponent(userId)}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          message: userMessage.content,
          conversation_history: messages.map(msg => ({
            role: msg.role,
            content: msg.content,
            timestamp: msg.timestamp,
          })),
          session_id: sessionId,
        }),
      });

      if (!response.ok) {
        throw new Error('Failed to send message');
      }

      const data = await response.json();

      if (!sessionId) {
        setSessionId(data.session_id);
      }

      // Simulate streaming effect by gradually revealing the text
      const fullResponse = data.response;
      let currentIndex = 0;
      
      const streamInterval = setInterval(() => {
        if (currentIndex >= fullResponse.length) {
          clearInterval(streamInterval);
          // Update final message with tool usage
          setMessages(prev => 
            prev.map((msg, index) => 
              index === prev.length - 1 
                ? { 
                    ...msg, 
                    content: fullResponse, 
                    isStreaming: false,
                    toolUsage: data.tool_usage 
                  }
                : msg
            )
          );
          setIsLoading(false);
          return;
        }
        
        // Add 1-3 characters at a time for more natural streaming
        const charsToAdd = Math.min(Math.floor(Math.random() * 3) + 1, fullResponse.length - currentIndex);
        currentIndex += charsToAdd;
        
        setMessages(prev => 
          prev.map((msg, index) => 
            index === prev.length - 1 
              ? { ...msg, content: fullResponse.substring(0, currentIndex) }
              : msg
          )
        );
      }, 30); // Adjust speed as needed
      
    } catch (error) {
      console.error('Error sending message:', error);
      // Remove streaming message and add error message
      setMessages(prev => {
        const newMessages = prev.slice(0, -1);
        const errorMessage: Message = {
          role: 'assistant',
          content: 'Sorry, there was an error processing your message. Please try again.',
          timestamp: new Date().toISOString(),
        };
        return [...newMessages, errorMessage];
      });
      setIsLoading(false);
    }
  };

  const handleKeyDown = (e: React.KeyboardEvent) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      sendMessage();
    }
  };

  const clearConversation = () => {
    setMessages([]);
    setSessionId(null);
  };

  const formatTime = (timestamp: string) => {
    return new Date(timestamp).toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
  };

  const renderToolUsage = (toolUsage: any) => {
    if (!toolUsage?.tool_calls?.length) return null;

    return (
      <div className="mb-3 p-3 glass rounded-lg border border-mono-200">
        <div className="flex items-center gap-2 mb-2">
          <svg className="w-4 h-4 text-mono-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M10.325 4.317c.426-1.756 2.924-1.756 3.35 0a1.724 1.724 0 002.573 1.066c1.543-.94 3.31.826 2.37 2.37a1.724 1.724 0 001.065 2.572c1.756.426 1.756 2.924 0 3.35a1.724 1.724 0 00-1.066 2.573c.94 1.543-.826 3.31-2.37 2.37a1.724 1.724 0 00-2.572 1.065c-.426 1.756-2.924 1.756-3.35 0a1.724 1.724 0 00-2.573-1.066c-1.543.94-3.31-.826-2.37-2.37a1.724 1.724 0 00-1.065-2.572c-1.756-.426-1.756-2.924 0-3.35a1.724 1.724 0 001.066-2.573c-.94-1.543.826-3.31 2.37-2.37.996.608 2.296.07 2.572-1.065z" />
            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M15 12a3 3 0 11-6 0 3 3 0 016 0z" />
          </svg>
          <span className="text-mono-700 font-medium text-xs">Tools used:</span>
        </div>
        <div className="space-y-1">
          {toolUsage.tool_calls.map((tool: any, index: number) => (
            <div key={index} className="text-xs font-mono text-mono-600 bg-mono-100 px-2 py-1 rounded">
              {tool.function?.name}
            </div>
          ))}
        </div>
      </div>
    );
  };

  return (
    <>
      {/* Floating Chat Button */}
      {!isOpen && (
        <button
          onClick={() => setIsOpen(true)}
          className="fixed bottom-4 right-4 sm:bottom-6 sm:right-6 z-50 w-14 h-14 sm:w-16 sm:h-16 bg-black hover:bg-gray-800 focus:bg-gray-800 text-white rounded-2xl shadow-2xl hover:shadow-xl focus:shadow-xl transition-all duration-300 hover:scale-105 focus:scale-105 focus:outline-none focus:ring-4 focus:ring-gray-300 flex items-center justify-center group animate-float"
          aria-label="Open Task Agent"
        >
          <svg className="w-8 h-8 transition-transform group-hover:scale-110" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M8 12h.01M12 12h.01M16 12h.01M21 12c0 4.418-4.03 8-9 8a9.863 9.863 0 01-4.255-.949L3 20l1.395-3.72C3.512 15.042 3 13.574 3 12c0-4.418 4.03-8 9-8s9 3.582 9 8z" />
          </svg>
          {userMessageCount > 0 && (
            <div className="absolute -top-2 -right-2 w-6 h-6 bg-red-500 text-white text-xs rounded-full flex items-center justify-center font-bold animate-bounce-in animate-pulse">
              {userMessageCount}
            </div>
          )}
        </button>
      )}

      {/* Backdrop */}
      {isOpen && (
        <div className="absolute bottom-16 right-0 w-80 bg-white rounded-lg shadow-xl border border-gray-200">
          {/* Chat header */}
          <div className="flex justify-between items-center p-3 border-b border-gray-200">
            <h3 className="font-semibold text-gray-800">TaskBoard Assistant</h3>
            <button
              onClick={() => setIsOpen(false)}
              className="text-gray-500 hover:text-gray-700"
            >
              ×
            </button>
          </div>

          {/* Messages area */}
          <div className="h-64 overflow-y-auto p-3 space-y-2">
            {messages.length === 0 && (
              <p className="text-gray-500 text-sm">Ask me to help with your tasks!</p>
            )}
            {messages.map((message, index) => (
              <div key={index} className={`${message.role === 'user' ? 'text-right' : 'text-left'}`}>
                <div className={`inline-block p-2 rounded-lg max-w-xs ${
                  message.role === 'user'
                    ? 'bg-blue-600 text-white'
                    : 'bg-gray-100 text-gray-800'
                }`}>
                  {message.content}
                </div>
              </div>
            </div>
            <div className="flex items-center gap-2">
              <button
                onClick={clearConversation}
                className="p-2 text-gray-500 hover:text-black hover:bg-gray-100 rounded-lg transition-all duration-200"
                title="Clear conversation"
              >
                <svg className="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 7l-.867 12.142A2 2 0 0116.138 21H7.862a2 2 0 01-1.995-1.858L5 7m5 4v6m4-6v6m1-10V4a1 1 0 00-1-1h-4a1 1 0 00-1 1v3M4 7h16" />
                </svg>
              </button>
              <button
                onClick={() => {
                  setIsOpen(false);
                  onChatClose?.();
                }}
                className="p-2 text-gray-500 hover:text-black hover:bg-gray-100 rounded-lg transition-all duration-200"
                title="Close chat"
              >
                <svg className="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M6 18L18 6M6 6l12 12" />
                </svg>
              </button>
            </div>
          </div>

          {/* Messages */}
          <div className="flex-1 overflow-y-auto p-3 sm:p-4 space-y-3 sm:space-y-4 min-h-0">
            {messages.length === 0 ? (
              <div className="text-center py-8 animate-fade-in">
                <div className="w-16 h-16 mx-auto mb-4 rounded-2xl bg-gray-100 flex items-center justify-center">
                  <svg className="w-8 h-8 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9.663 17h4.673M12 3v1m6.364 1.636l-.707.707M21 12h-1M4 12H3m3.343-5.657l-.707-.707m2.828 9.9a5 5 0 117.072 0l-.548.547A3.374 3.374 0 0014 18.469V19a2 2 0 11-4 0v-.531c0-.895-.356-1.754-.988-2.386l-.548-.547z" />
                  </svg>
                </div>
                <h3 className="text-base font-semibold text-gray-900 mb-2">Task Assistant</h3>
                <div className="text-sm text-gray-600 max-w-xs mx-auto space-y-2">
                  <p>Connected via <strong>Gram MCP</strong> - I can help you manage tasks, analyze your workflow, and boost productivity.</p>
                  <div className="text-xs text-gray-500 pt-2 border-t border-gray-100">
                    <p className="mb-1"><strong>Try:</strong></p>
                    <p>"Create a task for reviewing code"</p>
                    <p>"Show me my task progress"</p>
                    <p>"Help organize my workflow"</p>
                  </div>
                </div>
              </div>
            ) : (
              messages.map((message, index) => (
                <div
                  key={index}
                  className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'} animate-fade-in`}
                  style={{ animationDelay: `${index * 50}ms` }}
                >
                  <div
                    className={`max-w-[80%] px-4 py-3 rounded-2xl ${
                      message.role === 'user'
                        ? 'bg-black text-white'
                        : 'bg-gray-100 text-black border border-gray-200'
                    }`}
                  >
                    {message.role === 'assistant' && renderToolUsage(message.toolUsage)}
                    <div className="text-sm leading-relaxed">
                      {message.role === 'user' ? (
                        <p className="whitespace-pre-wrap">{message.content}</p>
                      ) : (
                        <div className={`prose prose-sm max-w-none ${
                          message.role === 'assistant' ? 'prose-gray' : ''
                        }`}>
                          <ReactMarkdown 
                            remarkPlugins={[remarkGfm]}
                            components={{
                              p: ({children}) => <p className="mb-2 last:mb-0">{children}</p>,
                              code: ({children, className}) => {
                                const isInline = !className;
                                return isInline ? (
                                  <code className="bg-gray-200 px-1 py-0.5 rounded text-xs font-mono">{children}</code>
                                ) : (
                                  <code className="block bg-gray-200 p-2 rounded text-xs font-mono whitespace-pre-wrap">{children}</code>
                                );
                              },
                              ul: ({children}) => <ul className="list-disc pl-4 mb-2">{children}</ul>,
                              ol: ({children}) => <ol className="list-decimal pl-4 mb-2">{children}</ol>,
                              li: ({children}) => <li className="mb-1">{children}</li>,
                              h1: ({children}) => <h1 className="text-lg font-bold mb-2">{children}</h1>,
                              h2: ({children}) => <h2 className="text-base font-bold mb-2">{children}</h2>,
                              h3: ({children}) => <h3 className="text-sm font-bold mb-1">{children}</h3>,
                              blockquote: ({children}) => <blockquote className="border-l-4 border-gray-300 pl-3 italic mb-2">{children}</blockquote>,
                              strong: ({children}) => <strong className="font-semibold">{children}</strong>,
                              em: ({children}) => <em className="italic">{children}</em>,
                            }}
                          >
                            {message.content}
                          </ReactMarkdown>
                          {message.isStreaming && (
                            <div className="flex items-center space-x-1 mt-2">
                              <div className="flex space-x-1">
                                <div className="w-2 h-2 bg-gray-500 rounded-full animate-bounce"></div>
                                <div className="w-2 h-2 bg-gray-500 rounded-full animate-bounce" style={{animationDelay: '0.1s'}}></div>
                                <div className="w-2 h-2 bg-gray-500 rounded-full animate-bounce" style={{animationDelay: '0.2s'}}></div>
                              </div>
                            </div>
                          )}
                        </div>
                      )}
                    </div>
                  </div>
                </div>
              ))
            )}
            <div ref={messagesEndRef} />
          </div>

          {/* Input Area */}
          <div className="flex-shrink-0 p-3 sm:p-4 bg-white/90 backdrop-blur-sm border-t border-gray-200">
            <div className="flex items-stretch h-11 sm:h-12 border-2 border-gray-200 rounded-xl bg-white/60 backdrop-blur-sm focus-within:border-black focus-within:bg-white/80 transition-all duration-200">
              <textarea
                ref={textareaRef}
                value={input}
                onChange={(e) => setInput(e.target.value)}
                onKeyDown={handleKeyDown}
                placeholder="Ask me about your tasks..."
                disabled={isLoading}
                className="flex-1 resize-none bg-transparent px-3 sm:px-4 py-2.5 sm:py-3 text-sm placeholder:text-gray-500 focus:outline-none disabled:bg-gray-100 disabled:cursor-not-allowed overflow-hidden"
                rows={1}
                style={{ minHeight: '44px', maxHeight: '120px' }}
              />
              <button
                onClick={sendMessage}
                disabled={!input.trim() || isLoading}
                className="flex-shrink-0 w-11 sm:w-12 bg-black text-white rounded-r-xl flex items-center justify-center hover:bg-gray-800 disabled:bg-gray-300 disabled:cursor-not-allowed transition-all duration-200 hover:scale-105 disabled:hover:scale-100 border-l border-gray-300"
                title="Send message"
              >
                {isLoading ? (
                  <div className="w-5 h-5 border-2 border-white border-t-transparent rounded-full animate-spin"></div>
                ) : (
                  <svg className="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M12 19l9 2-9-18-9 18 9-2zm0 0v-8" />
                  </svg>
                )}
              </button>
            </div>
          </div>
        </div>
      )}
    </>
  );
};

export default ChatComponent;
```

For the full code, refer to the TaskBoard repository's [`ChatComponent.tsx`](https://github.com/ritza-co/TaskBoard/blob/completed-ai-chat/taskboard/src/components/ChatComponent.tsx) file.

### Adding the chat component to your dashboard

Now integrate the chat component into your main dashboard:

```typescript
// taskboard/src/app/dashboard/page.tsx
// Add this import at the top
import ChatComponent from '@/components/ChatComponent';

// Inside your dashboard component, add this after the TaskModal component at the end of the final return statement:
{/* Floating Chat Component */}
{userId && <ChatComponent userId={userId} onChatClose={() => fetchItems(true)} />}
```

### Required dependencies

Make sure to install the required dependencies for the chat component:

```bash
npm install react-markdown remark-gfm
```

### CSS styling

The ChatComponent uses custom CSS classes and animations that need to be defined in your global stylesheet. For the complete CSS setup refer to the TaskBoard repository's [`globals.css`](https://github.com/ritza-co/TaskBoard/blob/completed-ai-chat/taskboard/src/app/globals.css) file.

Copy the relevant styles from the repository to ensure your chat component renders the generated text correctly.

At this point, you should have a chat interface that looks functional but doesn't connect to AI yet. The next step is adding the backend to make it work.


## Step 3: Creating the chat microservice

Now we'll create a small FastAPI service (about 100 lines of code) that handles the AI processing. This service uses OpenAI's Agents SDK.

**What this service does**:
1. Receives chat messages from your TaskBoard frontend
2. Creates an OpenAI Agent that has access to TaskBoard MCP tools (via Gram)
3. Agent decides which tools to use based on user intent
4. Returns natural language responses with the results

The OpenAI Agents SDK handles understanding user intent, choosing which MCP tools to call to achieve the user's intent, and generating responses based on the results of the MCP tools.

### Set up the chat service

Create the chat service directory:

```bash
# From the TaskBoard root directory
mkdir mcp-agent-service
cd mcp-agent-service
```

Create the requirements file:

```txt
# mcp-agent-service/requirements.txt
openai-agents
fastapi==0.116.1
uvicorn==0.35.0
python-dotenv==1.1.1
pydantic==2.11.7
httpx==0.28.1
requests==2.32.4
```

Create the main FastAPI service. **Important:** Replace `https://app.getgram.ai/mcp/your-server-id` on line 525 with your actual MCP server URL from Step 1:

```python
# mcp-agent-service/main.py
import os
import uuid
import asyncio
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
# OpenAI Agents SDK - handles the AI agent logic
from agents import Agent, Runner, SQLiteSession
from agents.mcp import MCPServerStdio  # Connects to Gram MCP server

app = FastAPI(title="OpenAI Agents Chat Microservice", version="1.0.0")

# Validate required environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TASKBOARD_SERVER_URL = os.getenv("TASKBOARD_SERVER_URL")

if not TASKBOARD_SERVER_URL:
    raise ValueError("TASKBOARD_SERVER_URL environment variable is required")

if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY environment variable is required")

# Set OpenAI API key for the library
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# Add CORS middleware to allow requests from Next.js app
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str
    conversation_history: List[Dict[str, Any]] = []
    session_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    session_id: str
    user_message_count: int
    tool_usage: Optional[Dict[str, Any]] = None

def extract_user_id_from_history(conversation_history: List[Dict[str, Any]]) -> Optional[str]:
    """Extract user ID from system message in conversation history."""
    for msg in conversation_history:
        if msg.get('role') == 'system' and msg.get('content'):
            content = msg['content']
            if 'User ID:' in content:
                return content.split('User ID:')[1].split('.')[0].strip()
    return None

async def create_mcp_server() -> MCPServerStdio:
    """Create MCP server connection to Gram.

    This connects to your Gram MCP server, which acts as a bridge
    between the OpenAI Agent and your TaskBoard API.
    """
    return MCPServerStdio(
        name="GramTaskboard",
        params={
            "command": "npx",  # Uses the mcp-remote package
            "args": [
                "mcp-remote",
                "https://app.getgram.ai/mcp/your-server-id",  # Replace with your actual Gram MCP server URL from Step 1
                "--header",
                f"MCP-TASKBOARD-SERVER-URL:{TASKBOARD_SERVER_URL}",  # Tells Gram where to find your API
                "--timeout", "120000"
            ]
        },
        cache_tools_list=True,  # Cache for performance
        client_session_timeout_seconds=120.0,
        max_retry_attempts=2,
        retry_backoff_seconds_base=2.0
    )

async def create_agent_with_mcp(user_id: str, mcp_server: MCPServerStdio) -> Agent:
    """Create an OpenAI Agent that knows how to use your TaskBoard API.

    The Agent automatically discovers your API endpoints via the MCP server
    and learns how to call them based on user requests.
    """
    return Agent(
        name="TaskBoard Assistant",
        instructions=f"""You are a helpful assistant that can manage tasks in a TaskBoard application.
        You can list, create, update, delete, and search for tasks using the available MCP tools.

        When interacting with tasks:
        - Always use the provided MCP tools to perform TaskBoard operations
        - Be helpful and provide clear responses about task operations
        - If a user wants to move a task, update its status (todo, doing, done)
        - When listing tasks, organize them by status if helpful
        - When creating tasks, ask for clarification if title or description is unclear

        {f'The user ID is: {user_id}. Always use this user ID when calling TaskBoard tools.' if user_id else 'User ID not available - tools may not work properly.'}

        Available task statuses: todo, doing, done

        Use natural language to explain what actions you're taking and their results.""",
        model="gpt-4o-mini",  # The LLM that powers the agent
        mcp_servers=[mcp_server]  # Your TaskBoard API tools
    )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    # Generate session ID if not provided
    session_id = request.session_id or str(uuid.uuid4())

    # Count user messages in conversation history
    user_message_count = 1  # Current message
    for msg in request.conversation_history:
        if msg.get('role') == 'user':
            user_message_count += 1

    # Check if we've reached the limit (5 messages)
    if user_message_count > 5:
        return ChatResponse(
            response="This conversation has reached the maximum limit of 5 user messages. Please start a new conversation.",
            session_id=session_id,
            user_message_count=user_message_count
        )

    # Extract userId from conversation history
    user_id = extract_user_id_from_history(request.conversation_history)

    try:
        # Create MCP server connection
        mcp_server = await create_mcp_server()

        # Use the MCP server with timeout
        async with asyncio.timeout(180):  # 3 minute total timeout
            async with mcp_server as server:
                # Create agent with the connected MCP server
                agent = await create_agent_with_mcp(user_id, server)

                # Create session for conversation persistence
                session = SQLiteSession(session_id, "conversations.db")

                # Run the agent
                result = await Runner.run(
                    agent,
                    request.message,
                    session=session
                )

                # Extract tool usage information
                tool_usage = None
                if hasattr(result, 'tool_calls') and result.tool_calls:
                    tool_usage = {
                        "has_tools": True,
                        "tool_calls": [
                            {
                                "function": {
                                    "name": tc.get('name', 'unknown'),
                                    "arguments": tc.get('arguments', {})
                                },
                                "content": tc.get('result', 'No result')
                            } for tc in result.tool_calls
                        ]
                    }

                return ChatResponse(
                    response=result.final_output,
                    session_id=session_id,
                    user_message_count=user_message_count,
                    tool_usage=tool_usage
                )

    except asyncio.TimeoutError:
        return ChatResponse(
            response="The TaskBoard service is taking too long to respond. Please try again in a few minutes.",
            session_id=session_id,
            user_message_count=user_message_count
        )
    except Exception as e:
        print(f"Error in chat endpoint: {e}")
        return ChatResponse(
            response="I'm experiencing technical difficulties. Please try again in a moment.",
            session_id=session_id,
            user_message_count=user_message_count
        )

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "openai-agents-chat-microservice"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8085)
```

### How the agent works

The OpenAI Agents SDK handles the complexity:

1. **Agent receives tools**: When you connect it to the MCP server, it gets access to TaskBoard tools (like "get_items", "create_item", etc.)
2. **Agent plans actions**: When a user says "show overdue tasks", it decides which MCP tools to call
3. **Agent executes**: It calls the appropriate MCP tools, which in turn call your `/api/items` endpoint
4. **Agent formats response**: It takes the raw JSON response and turns it into natural language

You don't write any prompt engineering, tool calling logic, or response formatting. The SDK handles it all.

### Set up environment variables

Create a `.env` file inside the `mcp-agent-service` directory for the Python service. Replace `your_openai_api_key_here` with your actual OpenAI API key and `https://your-ngrok-url.ngrok.io` with your ngrok URL from Step 1:

```bash
# mcp-agent-service/.env
OPENAI_API_KEY=your_openai_api_key_here
TASKBOARD_SERVER_URL=https://your-ngrok-url.ngrok.io  # Your ngrok URL from Step 1
```

## Step 4: Connect the frontend to the chat service

Finally we'll create the API route that connects your chat frontend to the chat service, ensuring user authentication is preserved.

### Add the chat API route

Create the chat API endpoint in your Next.js app:

```typescript
// taskboard/src/app/api/chat/route.ts
import { NextRequest, NextResponse } from 'next/server';

const CHAT_SERVICE_URL = process.env.CHAT_SERVICE_URL || 'http://localhost:8085';

export async function POST(request: NextRequest) {
  // Get userId from URL parameter (handled by middleware)
  let userId = request.headers.get('x-user-id');

  if (!userId) {
    userId = new URL(request.url).searchParams.get('userId');
  }

  if (!userId) {
    return NextResponse.json({ message: 'Unauthorized - userId required' }, { status: 401 });
  }

  try {
    const body = await request.json();

    // Prepare conversation history with user context
    const conversationHistory = body.conversation_history || [];

    // Add system message with user ID for security
    conversationHistory.unshift({
      role: 'system',
      content: `User ID: ${userId}. Always use this user ID when calling TaskBoard tools to ensure you access the correct user's data.`,
      timestamp: new Date().toISOString()
    });

    const requestBody = {
      message: body.message,
      conversation_history: conversationHistory,
      session_id: body.session_id,
    };

    // Forward to chat service
    const chatResponse = await fetch(`${CHAT_SERVICE_URL}/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(requestBody),
    });

    if (!chatResponse.ok) {
      throw new Error(`Chat service responded with status: ${chatResponse.status}`);
    }

    const chatData = await chatResponse.json();
    return NextResponse.json(chatData);

  } catch (error) {
    console.error('Chat API error:', error);
    return NextResponse.json(
      { message: 'Failed to process chat request' },
      { status: 500 }
    );
  }
}
```

### Update your middleware

Update sure your middleware includes the new chat route. Update this section of `middleware.ts` in the demo code:

```typescript
// taskboard/src/middleware.ts
// Update the config at the bottom of the file:
export const config = {
  matcher: ['/api/items', '/api/items/:path*', '/api/chat'], // Add /api/chat
};
```

## Docker setup

Now we'll set up Docker to run both services together. This is the recommended way to test the complete chat-enabled application.

Create the Dockerfile inside the `mcp-agent-service` directory:

```dockerfile
# mcp-agent-service/Dockerfile
FROM python:3.11-slim

# Install Node.js and npm for npx command
RUN apt-get update && apt-get install -y \
    curl \
    && curl -fsSL https://deb.nodesource.com/setup_20.x | bash - \
    && apt-get install -y nodejs \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .

EXPOSE 8085
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8085"]
```

Update your `docker-compose.yml` file in the root TaskBoard directory to include both services:

```yaml
# docker-compose.yml
version: '3.8'

services:
  taskboard:
    build: ./taskboard
    ports:
      - "3000:3000"
    environment:
      - CHAT_SERVICE_URL=http://chat-service:8085
      - DATABASE_URL=${DATABASE_URL:-file:./prisma/dev.db}
    depends_on:
      - chat-service
    volumes:
      - ./taskboard/prisma/dev.db:/app/prisma/dev.db
    networks:
      - taskboard-network
    env_file:
      - .env

  chat-service:
    build: ./mcp-agent-service
    ports:
      - "8085:8085"
    environment:
      - PYTHONUNBUFFERED=1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TASKBOARD_SERVER_URL=${TASKBOARD_SERVER_URL:-http://taskboard:3000}
    networks:
      - taskboard-network
    env_file:
      - .env

networks:
  taskboard-network:
    driver: bridge
```

### Set up environment variables for Docker

Update the `.env` file in the root TaskBoard directory (same level as `docker-compose.yml`) for Docker Compose to use. Replace the placeholder values with your actual API key and ngrok URL:

```bash
# .env (in root TaskBoard directory)
DATABASE_URL="file:./prisma/dev.db" # Already present
OPENAI_API_KEY=your_openai_api_key_here # Add your OpenAI API key here
TASKBOARD_SERVER_URL=https://your-ngrok-url.ngrok.io  # Your ngrok URL from Step 1
```

Start both services from the root TaskBoard directory (where `docker-compose.yml` is located):

```bash
# From the root TaskBoard directory
docker-compose up --build
```

### Test the complete integration

Visit `http://localhost:3000`, log in, and click the chat button. Try these test queries:

- "Show me all my tasks"
- "Create a task called 'Test the chat feature'"
- "Move the chat feature task to doing"

You should see:
1. **Chat responses** appear in the chat window
2. **Task board updates** automatically when AI performs actions
3. **Only your tasks** are visible/editable (user permissions respected)

<video width="600" controls style="display: block; margin: 0 auto;">
  <source src="/videos/taskboard-new-task.mp4" type="video/mp4" />
    Your browser does not support the video tag.
</video>

## Troubleshooting

### Chat not responding
- Check that both services are running (`localhost:3000` and `localhost:8085`)
- Verify your OpenAI API key is valid
- Look for errors in both terminal windows

### AI can't access your tasks
- Confirm ngrok is running and you've updated the `TASKBOARD_SERVER_URL` in Gram
- Test your MCP server in the Gram Playground first
- Check that the user is properly authenticated in TaskBoard

### Permission errors
- Make sure the `userId` is being passed correctly through the chat flow
- Verify your API middleware is working: `curl "http://localhost:3000/api/items?userId=test"`

![Chat service logs displayed in the terminal](/img/blog/adding-ai-chat-to-your-app-guide/chat-service-logging.png)
