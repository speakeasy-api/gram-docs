---
title: "How to add an LLM chat assistant to an existing app with MCP"
description: "Transform any CRUD application into an LLM-powered chat interface using MCP and Gram: Add natural language capabilities without rebuilding the backend."
sidebar:
  order: 1
---

Imagine your users could simply ask, "Show me all tasks assigned to Sarah that are due this week" instead of clicking through filters and menus. What if users could create entries, update records, and manage data using natural language, all while respecting existing security and permissions?

This guide shows you how to add intelligent chat capabilities to any existing application without rebuilding the backend or compromising the security model. We'll use [Gram](https://getgram.ai) to turn your current API into tools that a large language model (LLM) can call via the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/overview). Then we'll build a chat interface that feels like magic to your users.

All the code in this guide comes from the [TaskBoard example app](https://github.com/ritza-co/TaskBoard), a fully functional Kanban board with an MCP-powered chat assistant.

![The TaskBoard user registration page](/img/blog/adding-ai-chat-to-your-app-guide/taskboard-register.png)

:::note[Live Demo]
Try a [live demo of the TaskBoard app with a chat assistant here](https://taskboard.abduldavids.co.za). 

@TODO: Change to gram or ritza hosted domain.
:::

## The integration we're building

We'll add chat capabilities that use MCP to TaskBoard, a basic Kanban CRUD application built with Next.js and FastAPI. TaskBoard has three columns (To Do, In Progress, and Complete) that hold tasks. Users can create, update, and delete tasks, and move them between columns. The app handles basic user authentication and task management.

![The TaskBoard user interface](/img/blog/adding-ai-chat-to-your-app-guide/taskboard-interface.png)

The floating chat assistant we'll add interacts directly with the TaskBoard API and can respond to requests like:

- "Create a task for the client presentation due next Friday"
- "Show me all tasks that are overdue" 
- "Move the database migration task to done"
- "What items are in my doing column?"

![TaskBoard with chat functionality](/img/blog/adding-ai-chat-to-your-app-guide/taskboard-with-chat.png)

The existing application logic stays completely unchanged. The assistant respects all existing user permissions and authentication, and it can only access tasks the logged-in user is authorized to see and modify. All we'll do is add a new way for users to interact with the API behind the app that they're already using.

### Prerequisites

To follow along, you'll need:

- An existing web application with a REST API. This guide uses the [example TaskBoard app](https://github.com/ritza-co/TaskBoard).
- A [Gram account](https://getgram.ai).
- An OpenAI API key. Get yours from the [OpenAI dashboard](https://platform.openai.com/account/api-keys).
- Python 3.11 or higher for the FastAPI microservice.

### Understanding the architecture

Here's a high-level look at what we'll build:

![Diagram of the MCP-powered chat integration architecture](/img/blog/adding-ai-chat-to-your-app-guide/architecture-diagram.png)

When a user types a message in the chat, a request is sent to the Next.js backend, which extracts the `userId` and forwards it to the FastAPI chat service. The chat service uses `mcp-agent` to make a tool call to the Gram MCP server, which in turn makes a request to the application's API. For local development, `ngrok` is used to expose the local API to the Gram MCP server.

The entire process happens securely: The `userId` is passed at each step, ensuring that the assistant only accesses data the user is authorized to see. Your existing authentication and permissions are fully respected.

## Building a Gram MCP server

Next, we'll upload the API documentation to Gram, which will automatically generate MCP tools that language models can understand and call. This process preserves the application's security and business logic.

### Documenting the API

First, create an OpenAPI document that describes the API endpoints. Gram uses the OpenAPI document to create an MCP server that is interoperable with the API. As our example app uses Next.js, we'll use JSDoc comments to automatically generate this document.

```bash
# Install OpenAPI generation tool for Next.js
npm install -D next-swagger-doc
```

Now create a script to generate the OpenAPI document. In the project root, create `generate-swagger.js`:

```javascript generate-swagger.js
const { createSwaggerSpec } = require('next-swagger-doc');
const fs = require('fs');
const path = require('path');

const spec = createSwaggerSpec({
  apiFolder: 'src/app/api', // For Next.js 13+ with app directory
  // apiFolder: 'pages/api', // For Next.js 12 and earlier with pages directory
  definition: {
    openapi: '3.0.0',
    info: {
      title: 'TaskBoard API',
      version: '1.0.0',
      description: 'API documentation for TaskBoard application',
    },
  },
});

// Ensure public directory exists
const publicDir = path.join(process.cwd(), 'public');
if (!fs.existsSync(publicDir)) {
  fs.mkdirSync(publicDir, { recursive: true });
}

// Write the spec file
const outputPath = path.join(publicDir, 'swagger.json');
fs.writeFileSync(outputPath, JSON.stringify(spec, null, 2));

console.log(`✅ OpenAPI spec generated at ${outputPath}`);
```

Add this script to `package.json`:

```json
{
  "scripts": {
    "generate-docs": "node generate-swagger.js"
  }
}
```

Add JSDoc comments to API routes with detailed descriptions:

```typescript app/api/items/route.ts
/**
 * @swagger
 * /api/items:
 *   get:
 *     summary: Get user's items
 *     description: Retrieve all items belonging to the authenticated user, optionally filtered by status
 *     tags: [Items]
 *     parameters:
 *       - name: userId
 *         in: query
 *         description: User ID (required for authentication)
 *         required: true
 *         schema:
 *           type: string
 *     responses:
 *       200:
 *         description: Successfully retrieved items
 *         content:
 *           application/json:
 *             schema:
 *               type: array
 *               items:
 *                 $ref: '#/components/schemas/Item'
 * components:
 *   schemas:
 *     Item:
 *       type: object
 *       properties:
 *         userId:
 *           type: string
 */
export async function GET(request: NextRequest) {
  // Your existing API logic
}

export async function POST(request: NextRequest) {
  // Your existing API logic
}
```

Generate the OpenAPI document:

```bash
npm run generate-docs
# Your OpenAPI document will be available at /public/swagger.json
```

You can also use [`x-gram` extensions](/concepts/openapi#using-the-x-gram-extension) in JSDoc comments to add context that helps the assistant understand the app's tools more accurately:

```typescript
/**
 * @swagger
 * /api/tasks:
 *   post:
 *     summary: Create a new task
 *     x-gram:
 *       name: create_task
 *       description: |
 *         <context>
 *           This endpoint creates a new Kanban task for the authenticated user.
 *           Tasks can be created in any status (todo, doing, done) and will be associated with the user's account.
 *         </context>
 *         <prerequisites>
 *           - userId parameter is required for authentication
 *           - Title, details, and status are required fields
 *         </prerequisites>
 *         <usage>
 *           - Use status=todo for new tasks that need to be started
 *           - Use status=doing for tasks currently in progress
 *           - Use status=done for completed tasks
 *           - Title should be concise (max 200 characters)
 *           - Details can include full description (max 1000 characters)
 *         </usage>
 */
```


### Creating an MCP server with Gram

Now we'll transform the API documentation into a hosted MCP server that the chat agent can use. Gram will read the OpenAPI document and automatically generate the tools the chat assistant needs.

#### Uploading the OpenAPI document

1. In the Gram dashboard, click **Your APIs** in the sidebar (under **Create**).
2. Click **+ New OpenAPI Source**.
3. Upload the OpenAPI document (the generated `public/swagger.json` from the previous section).
4. Name the API (for example, "TaskBoard"), toolset, and server slug (for example, "taskboard-demo").

<video width="600" controls>
  <source src="/videos/taskboard-mcp-create.mp4" type="video/mp4" />
    Your browser does not support the video tag.
</video>

Gram will parse the OpenAPI document and generate [tool definitions](/concepts/tool-definitions) for each TaskBoard endpoint.

Find the MCP server in the **MCP** tab.


#### Configuring authentication

For development and testing, we need to expose the local TaskBoard API so Gram can access it. We'll use [ngrok](https://ngrok.com/) to create a public tunnel:

```bash
# Install ngrok if you haven't already
brew install ngrok  # or download from ngrok.com

# Expose your local TaskBoard on port 3000
ngrok http 3000
```

This gives you a public URL like `https://abc123.ngrok.io` that tunnels to your local `localhost:3000`.

In your default environment variables, set the API base URL to the ngrok URL (for example, `https://abc123.ngrok.io`).

![TaskBoard environment variables dialog](/img/blog/adding-ai-chat-to-your-app-guide/taskboard-env-var.png)


:::tip[Note]
ngrok is perfect for development and testing. However, in production, the TaskBoard API should be publicly accessible at a permanent URL that Gram can reach.
:::


#### Testing the toolset

Test the TaskBoard toolset in the Gram playground:

1. Go to the **Playground** tab in the sidebar and select the TaskBoard toolset.
2. Try natural language queries like:
   - "Show me all my tasks"
   - "Create a task called 'Review quarterly reports'"

![Testing the TaskBoard MCP tools in the Gram Playground](/img/blog/adding-ai-chat-to-your-app-guide/testing-mcp-taskboard.png)

:::tip[Note]
You'll need to manually provide the `userId` parameter in the Playground for the agent to access the correct user's data. We'll automate this in the actual chat service.

Alternatively, you can ask the agent to use the login and register tools to manually login and get a `userId` token.
:::

#### Deploying the toolset as a hosted MCP server

Go to the **MCP** tab in the sidebar and select the TaskBoard toolset. Scroll down to the **Visibility** section and change the MCP server to **Public**.

![Setting the MCP server to public in the Gram MCP tab](/img/blog/adding-ai-chat-to-your-app-guide/gram-public-mcp-server.png)


The toolset is now available as a hosted MCP server. Scroll down to the **MCP Config** section to get the server configuration. It will look something like this:

```json
{
  "mcpServers": {
    "GramTaskboard": {
      "command": "npx",
      "args": [
        "mcp-remote",
        "https://app.getgram.ai/mcp/<your-mcp-server-slug>",
        "--header",
        "MCP-TASKBOARD-SERVER-URL:${TASKBOARD_SERVER_URL}"
      ]
    }
  }
}
```

Save this configuration. We'll use it when we set up the chat service. 

## Adding chat capabilities to the application

We'll build a chat interface that floats over the TaskBoard app, and connect it to the MCP server through a Python microservice. The FastAPI service handles the chat logic and tool execution, so we won't need to rewrite the existing Next.js frontend apart from adding the chat UI.

### Building the FastAPI chat service

For the agent itself, we'll use the [`mcp-agent`](https://github.com/lastmile-ai/mcp-agent) library. 

First, create a new directory for the chat service:

```bash
mkdir mcp-agent-service
cd mcp-agent-service
```

In the `mcp-agent-service` directory, create a basic FastAPI service that uses `mcp-agent` to connect to the Gram MCP server:

```python
import os
from fastapi import FastAPI
from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM
# ... other imports (uuid, pydantic, CORS, etc.) ...

app = FastAPI(title="MCP Chat Microservice")
mcp_app = MCPApp(name="mcp_fastapi_chat_app")

# ... environment validation, CORS setup, request/response models ...

@app.post("/chat")
async def chat(request: ChatRequest):
    # ... session management, message counting, rate limiting ...
    
    # Extract userId from system message - crucial for security
    user_id = None
    for msg in request.conversation_history:
        if msg.get('role') == 'system' and 'User ID:' in msg.get('content', ''):
            user_id = msg['content'].split('User ID:')[1].split('.')[0].strip()
            break
    
    async with mcp_app.run():
        # Create MCP agent with user context
        chat_agent = Agent(
            name="chat_agent",
            instruction=f"""You are a helpful assistant.
            {f'The user ID is: {user_id}. Always use this user ID when calling TaskBoard tools.' if user_id else ''}""",
            server_names=["GramTaskboard"],  # Connects to Gram MCP server
        )

        async with chat_agent:
            # Set up LLM and generate response
            llm = await chat_agent.attach_llm(
                llm_factory=lambda agent: OpenAIAugmentedLLM(agent=agent, default_model="gpt-4o-mini")
            )
            
            result = await llm.generate_str(message=f"User: {request.message}")
            
            # ... tool usage extraction and response formatting ...
            
            return ChatResponse(response=result, session_id=session_id, ...)
```

#### Configuring environment variables

Next, create a `.env` file to store environment variables. You can get your OpenAI API key from the [OpenAI dashboard](https://platform.openai.com/account/api-keys).

```env
OPENAI_API_KEY=<YOUR_OPENAI_API_KEY>
TASKBOARD_SERVER_URL=<YOUR_API_BASE_URL> # replace with your public API base URL
```

#### Configuring the MCP agent

Now create an `mcp_agent.config.yaml` file to configure the chat agent. This file tells the agent how to connect to the Gram MCP server and how to use the OpenAI API. For more information on the configuration, see the [`mcp-agent` documentation](https://github.com/lastmile-ai/mcp-agent).

```yaml
execution_engine: asyncio
logger:
  transports: [console]
  level: debug
  path: "logs/mcp-agent.jsonl"
mcp:
  servers:
    GramTaskboard:  # This matches the server name from the MCP Config section above
      command: "npx"
      args: [
        "mcp-remote",
        "https://app.getgram.ai/mcp/ritza-rzx-taskboard",  # Your Gram MCP server URL
        "--header",
        "MCP-TASKBOARD-SERVER-URL:${TASKBOARD_SERVER_URL}"  # Passes your ngrok URL to Gram
      ]
openai:
  api_key: "${OPENAI_API_KEY}"
  default_model: "gpt-4o-mini"
  reasoning_effort: "medium"
```

:::note[Note]
This configuration comes directly from the **MCP Config** you copied from Gram in the previous step. The server name **must** match what you used when creating the MCP agent in the Python code.
:::

### Building the chat frontend component

Next, we'll add a chat component to the app. For the TaskBoard example, we'll use a simple popup component that floats over the main interface. 

```tsx ChatPopup.tsx
const sendMessage = async () => {
    if (!input.trim() || isLoading || userMessageCount >= MAX_MESSAGES) return;

    const userMessage: Message = {
      role: 'user',
      content: input.trim(),
      timestamp: new Date().toISOString(),
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);

    // Add streaming placeholder
    const streamingMessage: Message = {
      role: 'assistant',
      content: '',
      timestamp: new Date().toISOString(),
      isStreaming: true,
    };
    setMessages(prev => [...prev, streamingMessage]);

    try {
      const response = await fetch(`/api/chat?userId=${encodeURIComponent(userId)}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: userMessage.content,
          conversation_history: messages.map(msg => ({
            role: msg.role,
            content: msg.content,
            timestamp: msg.timestamp,
          })),
          session_id: sessionId,
        }),
      });

      if (!response.ok) throw new Error('Failed to send message');
      const data = await response.json();

      if (!sessionId) setSessionId(data.session_id);

      // Simulate streaming effect
      const fullResponse = data.response;
      let currentIndex = 0;
      
      const streamInterval = setInterval(() => {
        if (currentIndex >= fullResponse.length) {
          clearInterval(streamInterval);
          setMessages(prev => 
            prev.map((msg, index) => 
              index === prev.length - 1 
                ? { ...msg, content: fullResponse, isStreaming: false, toolUsage: data.tool_usage }
                : msg
            )
          );
          setIsLoading(false);
          return;
        }
        
        const charsToAdd = Math.min(Math.floor(Math.random() * 3) + 1, fullResponse.length - currentIndex);
        currentIndex += charsToAdd;
        
        setMessages(prev => 
          prev.map((msg, index) => 
            index === prev.length - 1 
              ? { ...msg, content: fullResponse.substring(0, currentIndex) }
              : msg
          )
        );
      }, 30);
      
    } catch (error) {
      console.error('Error sending message:', error);
      // Handle error...
      setIsLoading(false);
    }
  };
```

See the [full component code on GitHub](https://github.com/ritza-co/TaskBoard/blob/main/taskboard/src/components/ChatPopup.tsx).

Now we'll create a Next.js API route that acts as a secure proxy between the frontend chat component and the Python microservice. This route handles user authentication and forwards the `userID` to the agent.

```typescript app/api/chat/route.ts
import { NextRequest, NextResponse } from 'next/server';

const CHAT_SERVICE_URL = process.env.CHAT_SERVICE_URL || 'http://localhost:8085';

export async function POST(request: NextRequest) {
  let userId = request.headers.get('x-user-id');
  
  if (!userId) {
    userId = new URL(request.url).searchParams.get('userId');
  }

  if (!userId) {
    return NextResponse.json({ message: 'Unauthorized' }, { status: 401 });
  }

  try {
    const body = await request.json();

    const conversationHistory = body.conversation_history || [];
    
    conversationHistory.unshift({
      role: 'system',
      content: 'User ID: ' + userId + '. Always use this user ID when calling TaskBoard tools to ensure you access the correct user\'s data.',
      timestamp: new Date().toISOString()
    });

    const requestBody = {
      message: body.message,
      conversation_history: conversationHistory,
      session_id: body.session_id,
    };
    
    const chatResponse = await fetch(`${CHAT_SERVICE_URL}/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(requestBody),
    });

    if (!chatResponse.ok) {
      throw new Error(`Chat service responded with status: ${chatResponse.status}`);
    }

    const chatData = await chatResponse.json();

    return NextResponse.json(chatData);
  } catch (error) {
    console.error('Chat API error:', error);
    return NextResponse.json(
      { message: 'Failed to process chat request' },
      { status: 500 }
    );
  }
}
```

See the [full API route code on GitHub](https://github.com/ritza-co/TaskBoard/blob/main/taskboard/src/app/api/chat/route.ts).


### Integrating chat into the app

Let's wire everything together by adding the chat popup to the main application. We'll drop the popup into the TaskBoard dashboard, where it can float over the Kanban board without getting in the way.

```tsx
// components/ChatPopup.tsx (simplified from TaskBoard)
import React, { useState } from 'react';

const ChatPopup: React.FC = () => {
  const [isOpen, setIsOpen] = useState(false);

  const togglePopup = () => {
    setIsOpen(!isOpen);
  };

  return (
    <div>
      <button
        onClick={togglePopup}
        className="fixed bottom-8 right-8 bg-indigo-600 text-white p-4 rounded-full shadow-lg hover:bg-indigo-700"
      >
        <svg className="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
          <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} 
                d="M8 12h.01M12 12h.01M16 12h.01M21 12c0 4.418-4.03 8-9 8a9.863 9.863 0 01-4.255-.949L3 20l1.395-3.72C3.512 15.042 3 13.574 3 12c0-4.418 4.03-8 9-8s9 3.582 9 8z" />
        </svg>
      </button>

      {isOpen && (
        <div className="fixed bottom-24 right-8 w-80 bg-white rounded-lg shadow-lg border border-gray-200">
          <div className="flex justify-between items-center p-3 border-b border-gray-200">
            <h3 className="font-semibold">TaskBoard Assistant</h3>
            <button onClick={togglePopup} className="text-gray-500 hover:text-gray-700">
              ×
            </button>
          </div>
          <div className="p-4">
            <p className="text-sm text-gray-600">
              Connected via Gram MCP - I can help manage your tasks!
            </p>
            {/* Your chat implementation goes here */}
          </div>
        </div>
      )}
    </div>
  );
};

export default ChatPopup;
```

Integrate the popup into the main app:

```tsx Dashboard.tsx
import ChatPopup from '@/components/ChatPopup';

export default function Dashboard() {
  return (
    <div className="min-h-screen bg-gray-50">
      {/* existing TaskBoard content */}
      <YourKanbanBoard />
      
      {/* Floating chat component */}
      <ChatPopup />
    </div>
  );
}
```

### Dockerizing the application

Now let's containerize both services so they can run alongside the existing application. We'll define a Docker container for the chat service and add it to the example app's `docker-compose.yml` file.

In the `mcp-agent-service` directory, create a `Dockerfile` to containerize the chat service.

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create logs directory
RUN mkdir -p logs

EXPOSE 8085

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8085"]
```

Update the top-level `docker-compose.yml` file to include the chat service.

```yaml docker-compose.yml
version: '3.8'
services:
  taskboard:
    build: ./taskboard
    ports:
      - "3000:3000"
    environment:
      - DATABASE_URL=file:./dev.db
      - CHAT_SERVICE_URL=http://chat-service:8085
    depends_on:
      - chat-service
    networks:
      - taskboard-network

  chat-service:
    build: ./chat-service
    ports:
      - "8085:8085"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TASKBOARD_SERVER_URL=http://taskboard:3000
    volumes:
      - ./chat-service/logs:/app/logs
    networks:
      - taskboard-network

networks:
  taskboard-network:
    driver: bridge
```

#### Environment configuration

Now update the `.env` file to include the chat service URL, the MCP server URL, and the OpenAI API key. Docker will automatically pick up these variables and use them in the chat service.

```env
OPENAI_API_KEY=your_openai_api_key_here
CHAT_SERVICE_URL=http://localhost:8085
# For development: use your ngrok URL so Gram can access your local API
TASKBOARD_SERVER_URL=https://abc123.ngrok.io
# For production: use your actual public API URL
# TASKBOARD_SERVER_URL=https://your-app.com
```

### Testing the integration

Time to see the chat assistant in action! With everything dockerized, let's test the complete setup.

#### Set up the ngrok tunnel (for development)

First, expose the TaskBoard API so Gram can access it:

```bash
# In a separate terminal, start the ngrok tunnel
ngrok http 3000

# Copy the public URL (like https://abc123.ngrok.io) 
# and update your .env file with this URL
```

We'll use an ngrok tunnel to expose the local API to the Gram MCP server. This is a temporary solution for development. In production, you should use the public API URL.

Start the services:

```bash
# Start all services with Docker
docker-compose up -d

# Check that everything is running
docker-compose ps

# Follow the logs
docker-compose logs -f
```

To test the new chat component, we'll log in to the app and create a new task, then open the chat component and ask the agent to list all the current tasks.

<video width="600" controls>
  <source src="/videos/taskboard-show-tasks.mp4" type="video/mp4" />
    Your browser does not support the video tag.
</video>

Notice how the agent only lists the tasks assigned to the logged-in user.

Now let's ask the agent to create a new task to review last week's PR from Jeff. To see how the agent handles coming up with more context for the details field, we'll make our request purposefully vague.

<video width="600" controls>
  <source src="/videos/taskboard-new-task.mp4" type="video/mp4" />
    Your browser does not support the video tag.
</video>

The agent creates a new task for us in the **To Do** column.

We can also ask the agent to delete a task.

![Using the TaskBoard chat to ask the agent to delete a task](/img/blog/adding-ai-chat-to-your-app-guide/taskboard-delete-task.png)


## Troubleshooting

Here are a few common issues and how to resolve them.

Chat not responding:

- Verify that all services are running.
- Check that the environment variables are set correctly.
- Look for errors in the browser developer console.

Assistant can't access data:

- Confirm that the MCP server is accessible. If you're using ngrok, make sure you're using the correct URL.
- Check that user authentication is working.
- Verify that the API endpoints are returning the expected data.

Permission errors:

- Ensure that the user ID is passed correctly.
- Check the API authentication middleware.
- Verify that CORS settings allow the frontend domain.


If you're having issues, enable logging in the chat service to see detailed information about how the agent uses tools.

```python main.py
import logging
logging.basicConfig(level=logging.INFO)

@app.post("/chat")
async def chat(request: ChatRequest):
    user_id = extract_user_id_from_history(request.conversation_history)
    logging.info(f"Chat request from user {user_id}")
    logging.info(f"Message: {request.message}")
    
    # ... processing ...
    
    if tool_usage and tool_usage.get('has_tools'):
        tool_names = [tc['function']['name'] for tc in tool_usage['tool_calls']]
        logging.info(f"Tools used: {tool_names}")
    
    return response
```

![Chat service logs displayed in the terminal](/img/blog/adding-ai-chat-to-your-app-guide/chat-service-logging.png)


## Next steps

Adding an LLM-powered chat assistant to an existing application doesn't require rebuilding the backend or learning complex AI frameworks. With MCP and Gram, you can create powerful natural language interfaces that work seamlessly with your current architecture.

Your users get intuitive new ways to interact with your app, while you maintain complete control over security, permissions, and functionality. The assistant becomes another interface to your existing system – one that speaks your users' language.

Ready to add a chat assistant to your app? Start by exploring the [TaskBoard example](https://github.com/ritza-co/TaskBoard), then create your first MCP server with [Gram](https://getgram.ai).
